# Dockerfile.rebase
# Build a test overlay image by layering PR source + vLLM runtime deps over a Neuron base.
ARG BASE_IMAGE
FROM ${BASE_IMAGE}

# Upgrade core packaging tools (safe; already high versions in base but idempotent)
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# Copy repo source so we can read requirements files
WORKDIR /workspace/src
COPY . /workspace/src

# --- Install vLLM deps -------------------------------------------------------
# We build filtered requirement files:
#   - Drop comments, blanks
#   - Drop nested -r includes (we explicitly include both top-level files)
#   - Drop torch*, torchvision*, torchaudio*, flash-attn* (Neuron stack provides torch-neuronx)
# Then install both filtered files together.
RUN set -e; \
    grep -v -E '^\s*(#|$|-r)' requirements/common.txt \
      | grep -v -Ei '^(torch|torchvision|torchaudio|flash[-_]attn)' > /tmp/vllm-common.txt; \
    grep -v -E '^\s*(#|$|-r)' requirements/neuron.txt \
      | grep -v -Ei '^(torch|torchvision|torchaudio|flash[-_]attn)' > /tmp/vllm-neuron.txt; \
    cat /tmp/vllm-common.txt /tmp/vllm-neuron.txt > /tmp/vllm-deps.txt; \
    echo "Installing vLLM deps from filtered list:"; \
    cat /tmp/vllm-deps.txt; \
    pip install --no-cache-dir -r /tmp/vllm-deps.txt || echo 'WARN: filtered requirements install returned non-zero (continuing PoC)'; \
    # Insurance: explicitly ensure critical runtime deps present / upgraded.
    pip install --no-cache-dir "pydantic>2" tenacity fastapi uvicorn sse-starlette

# Make repo importable without build (avoid CUDA toolchain requirement)
ENV PYTHONPATH=/workspace/src:${PYTHONPATH}

# Default smoke command (Kaizen will override)
CMD ["python", "examples/offline_inference/neuron.py"]
