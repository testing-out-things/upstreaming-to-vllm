# Lightweight overlay: layer PR vLLM (editable) on top of latest Neuron base.

ARG BASE_IMAGE
FROM ${BASE_IMAGE}

# Make sure we target the Neuron backend (skips CUDA checks during build).
ENV VLLM_TARGET_DEVICE=neuron

# Reduce pip noise and speed things up a bit.
ENV PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_NO_CACHE_DIR=1

# (Base images already include Python + Neuron toolchain; upgrade core tools is cheap & idempotent.)
RUN pip install --upgrade pip setuptools wheel

# Copy PR source.
WORKDIR /workspace/src
COPY . /workspace/src

# Install vLLM runtime deps for Neuron.
# We point pip at the Neuron extra index; the packages already in the base should satisfy most constraints,
# so pip will mostly no-op. Upgrade strategy "only-if-needed" avoids churning large wheels.
RUN pip install --upgrade-strategy only-if-needed \
      --extra-index-url=https://pip.repos.neuron.amazonaws.com \
      -r requirements/neuron.txt

# Some upstream images explicitly remove transformers-neuronx due to version conflicts; keep that here but ignore failure.
RUN pip uninstall -y transformers-neuronx || true

# Install *this PR's* vLLM in editable form but DO NOT resolve deps again.
# (Deps were handled above; this just drops an .egg-link so imports resolve to /workspace/src.)
RUN pip install --no-build-isolation --no-deps -v -e .

# Optional: fast smoke deps (if not already present via requirements)
RUN pip install fastapi uvicorn sse-starlette tenacity || true

# Make the repo importable even if something goes odd during install.
ENV PYTHONPATH=/workspace/src:${PYTHONPATH}

# Default smoke cmd (Kaizen overrides)
CMD ["python", "examples/offline_inference/neuron.py"]
